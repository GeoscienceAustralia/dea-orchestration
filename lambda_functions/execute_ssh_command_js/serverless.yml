service: execute-on

plugins:
  # install serverless pseudo parameters using the following:
  #   1) $ npm install serverless-pseudo-parameters --save-dev
  - serverless-pseudo-parameters

custom:
  Stage: ${opt:stage, self:provider.stage}
  profiles:
    dev: devProfile
    prod: prodProfile
  CustEnv:
    s3Bucket:
      prod: "dea-lambda"
      dev: "dea-lambdas-dev"

provider:
  name: aws
  runtime: nodejs10.x
  timeout: 600  # 10 minutes. Default is 6 seconds
  profile: ${self:custom.profiles.${self:custom.Stage}}
  environment:
    hostkey: 'orchestrator.raijin.users.default.host'
    userkey: 'orchestrator.raijin.users.default.user'
    pkey: 'orchestrator.raijin.users.default.pkey'
    webhook: 'orchestrator.raijin.users.default.slack.webhookurl'
    DEA_MODULE: dea/unstable
    PROJECT: v10
    QUEUE: normal
  region: ap-southeast-2
  deploymentBucket: ${self:custom.CustEnv.s3Bucket.${self:custom.Stage}}
  stackTags:
    repo: https://github.com/GeoscienceAustralia/dea-orchestration
    author: nci.monitor@dea.ga.gov.au
    purpose: nci-automation
  iamRoleStatements:
    - Effect: 'Allow'
      Action:
        - 'ssm:GetParameters'
        - 'ssm:DescribeParameters'
      Resource:
        - "arn:aws:ssm:#{AWS::Region}:#{AWS::AccountId}:parameter/orchestrator.*"
        - "arn:aws:ssm:#{AWS::Region}:#{AWS::AccountId}:parameter/pipeline.*"
    - Effect: 'Allow'
      Action: 'kms:Decrypt'
      Resource:
        - "arn:aws:kms:#{AWS::Region}:#{AWS::AccountId}:key/*"

functions:
  git_pull_prod:
    handler: handler.execute_ssh_command
    environment:
      hostkey: 'orchestrator.raijin.users.git_pull.host'
      userkey: 'orchestrator.raijin.users.git_pull.user'
      pkey: 'orchestrator.raijin.users.git_pull.pkey'
    events:
      - schedule:
          rate: cron(00 10 ? * SUN *)  # Run every Sunday, at 08:00 pm Canberra time
          enabled: false
  execute_sync:
    # trasharchived is set to 'yes' only for the albers products and not for the scenes
    handler: handler.execute_ssh_command
    description: Sync Sentinal 2 ARD Granules
    environment:
      cmd: 'execute_sync --dea-module ${self:provider.environment.DEA_MODULE}
                         --queue ${self:provider.environment.QUEUE}
                         --project ${self:provider.environment.PROJECT}
                         --year <%= year %>
                         --path <%= path %>
                         --suffixpath <%= suffixpath %>
                         --product <%= product %>
                         --trasharchived <%= trasharchived %>'
    events:
      - schedule:
          rate: cron(30 00 ? * WED *)  # Run every Wednesday, at 10:30 am Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2020-2020'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-01-'  # Process January month data
      - schedule:
          rate: cron(30 00 ? * WED *)  # Run every Wednesday, at 10:30 am Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2020-2020'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-02-'  # Process February month data
      - schedule:
          rate: cron(30 00 ? * WED *)  # Run every Wednesday, at 10:30 am Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2020-2020'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-03-'  # Process March month data
      - schedule:
          rate: cron(30 00 ? * WED *)  # Run every Wednesday, at 10:30 am Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2020-2020'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-04-'  # Process April month data
      - schedule:
          rate: cron(30 00 ? * WED *)  # Run every Wednesday, at 10:30 am Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-05-'  # Process May month data
      - schedule:
          rate: cron(30 00 ? * WED *)  # Run every Wednesday, at 10:30 am Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-06-'  # Process June month data
      - schedule:
          rate: cron(30 00 ? * WED *)  # Run every Wednesday, at 10:30 am Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-07-'  # Process July month data
      - schedule:
          rate: cron(30 00 ? * WED *)  # Run every Wednesday, at 10:30 am Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-08-'  # Process August month data
      - schedule:
          rate: cron(30 00 ? * WED *)  # Run every Wednesday, at 10:30 am Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-09-'  # Process September month data
      - schedule:
          rate: cron(30 00 ? * WED *)  # Run every Wednesday, at 10:30 am Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-10-'  # Process October month data
      - schedule:
          rate: cron(30 00 ? * WED *)  # Run every Wednesday, at 10:30 am Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-11-'  # Process November month data
      - schedule:
          rate: cron(30 00 ? * WED *)  # Run every Wednesday, at 10:30 am Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-12-'  # Process December month data
  execute_coherence:
    # Note: Keep `--timequery` argument as the last argument in the environment.cmd
    handler: handler.execute_ssh_command
    description: Execute dea-coherence tool to report archived/locationless parent and their derived datasets
    environment:
      cmd: 'execute_coherence --dea-module ${self:provider.environment.DEA_MODULE}
                              --queue ${self:provider.environment.QUEUE}
                              --project ${self:provider.environment.PROJECT}
                              --product <%= product %>
                              --timequery <%= timequery %>'
    events:
      - schedule:
          rate: cron(00 09 ? * TUE *)  # Run every Tuesday, at 07:00 pm Canberra time
          enabled: false
          input:
            timequery: 'time in 2019'
            product: all
  execute_notify_on_error_ds:
    handler: handler.execute_ssh_command
    description: Detect CSV file containing discrepencies in database, upload CSV to S3 and notify (Slack and Email)
    environment:
      cmd: 'execute_notify_on_error_ds --dea-module ${self:provider.environment.DEA_MODULE}
                                       --webhook  ${self:provider.environment.webhook}
                                       --awsprofile <%= awsprofile %>
                                       --recemail <%= recemail %>'


    events:
      - schedule:
          rate: cron(00 12 ? * * *)  # Run every day, at 10:00 pm Canberra time
          enabled: false
          input:
            awsprofile: default
            recemail: nci.monitor@dea.ga.gov.au  # Recepient's email address
  execute_clean:
    # Note: Keep `--search-paths` argument as the last argument in the environment.cmd
    handler: handler.execute_ssh_command
    description: Execute dea-clean tool to list things to be cleaned, not go and automatically delete files
    environment:
      cmd: 'execute_clean --dea-module ${self:provider.environment.DEA_MODULE}
                          --queue ${self:provider.environment.QUEUE}
                          --project ${self:provider.environment.PROJECT}
                          --min-trash-age <%= min_trash_age %>
                          --search-paths <%= search_paths %>'
    events:
      - schedule:
          rate: cron(00 13 ? * TUE *)  # Run every Tuesday, at 11:00 pm Canberra time
          enabled: false
          input:
            min_trash_age: 72  # 72 hours
            search_paths: /g/data/rs0/datacube/002/
              /g/data/fk4/datacube/002/FC/
              /g/data/fk4/datacube/002/WOfS/WOfS_25_2_1/netcdf/
  execute_s2ard:
    handler: handler.execute_ssh_command
    description: Entrypoints for Sentinel-2 level-1 yaml generation and level-2 processing
    environment:
      cmd: 'execute_s2ard --project ${self:provider.environment.PROJECT}
                          --level1-dir <%= level1_dir %>
                          --output-dir <%= output_dir %>
                          --copy-parent-dir-count <%= copy_parent_dir_count %>
                          --file-mod-start <% print(new Date().toISOString().split("T")[0]) %>
                          --file-mod-start-offset <%= start_date_offset %>
                          --days-to-process <%= days_to_process %>
                          --task <%= task %>
                          --obs-year <%= obs_year %>'
    events:
      - schedule:
          rate: cron(5 1 ? * FRI *)  # Run weekly at 11:05AM AEST
          enabled: false
          input:
            task: level1
            start_date_offset: 7  # Process last 7 days up until 00:00 Tuesday
            days_to_process: 7
            output_dir: /g/data/v10/AGDCv2/datacube-ingestion/indexed-products/cophub/s2/s2_l1c_yamls
            copy_parent_dir_count: 1
            level1_dir: /g/data/fj7/Copernicus/Sentinel-2/MSI/L1C
            obs_year: current
      - schedule:
          rate: cron(10 8 4 * ? *)  # Run on the 4th of every month at 6:10 PM AEST
          enabled: false
          input:
            task: level1
            start_date_offset: 200  # Kicks off re-processing for the past 200 days
            days_to_process: 200  # Number of days to process from the start offset
            output_dir: /g/data/v10/AGDCv2/datacube-ingestion/indexed-products/cophub/s2/s2_l1c_yamls
            copy_parent_dir_count: 1
            level1_dir: /g/data/fj7/Copernicus/Sentinel-2/MSI/L1C
            obs_year: current
      - schedule:
          rate: cron(5 1 ? * SUN *)  # Run every Sunday at 11:05 AM, AEST
          enabled: false
          input:
            task: level2
            start_date_offset: 12  # Process from Monday 00:00 to Monday 00:00
            days_to_process: 7
            output_dir: /g/data/if87/datacube/002/S2_MSI_ARD/packaged
            copy_parent_dir_count: 0  # IGNORED FOR L2
            level1_dir: /g/data/fj7/Copernicus/Sentinel-2/MSI/L1C
            obs_year: current
      - schedule:
          rate: cron(5 11 6 * ? *)  # Run on the 6th of every month at 9:05 PM AEST
          enabled: false
          input:
            task: level2
            start_date_offset: 31  # Kicks off re-processing for the past 31 days
            days_to_process: 32  # Number of days to process from the start offset
            output_dir: /g/data/if87/datacube/002/S2_MSI_ARD/packaged
            copy_parent_dir_count: 0  # IGNORED FOR L2
            level1_dir: /g/data/fj7/Copernicus/Sentinel-2/MSI/L1C
            obs_year: current
  execute_s2_to_s3_rolling:
    handler: handler.execute_ssh_command
    description: Entrypoints for running the Sentinel-2 to S3 sync. This should maintain a 2 year rolling archive.
    environment:
      cmd: 'execute_s2_to_s3_rolling
                          --profile <%= profile %>
                          --s3bucket <%= s3bucket %>
                          --numdays <%= numdays %>
                          --endate <%= enddate %>'
    events:
      - schedule:
          rate: cron(5 12 ? * SUN *)  # Run weekly on a Sunday 10 PM AEST
          enabled: true
          input:
            profile: default
            s3bucket: dea-public-data
            numdays: 14  # two weeks of data
            enddate: today  # must be 'today' or yyyy-mm-dd
