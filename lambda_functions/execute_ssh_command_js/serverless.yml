# Welcome to Serverless!
#
# This file is the main config file for your service.
#
# For full config options, check the docs:
#    docs.serverless.com
#
# Happy Coding!
service: execute-on

# You can pin your service to only deploy with a specific Serverless version
# Check out our docs for more details
# frameworkVersion: "=X.X.X"

# plugins:
plugins:
  # install serverless pseudo parameters using the following:
  #   1) $ npm install serverless-pseudo-parameters --save-dev
  - serverless-pseudo-parameters

custom:
  Stage: ${opt:stage, self:provider.stage}
  profiles:
    dev: devProfile
    prod: prodProfile
  CustEnv:
    s3Bucket:
      prod: "dea-lambda"
      dev: "dea-lambdas-dev"

provider:
  name: aws
  runtime: nodejs6.10
  timeout: 600  # 10 minutes. Default is 6 seconds
  profile: ${self:custom.profiles.${self:custom.Stage}}
  environment:
    hostkey: 'orchestrator.raijin.users.default.host'
    userkey: 'orchestrator.raijin.users.default.user'
    pkey: 'orchestrator.raijin.users.default.pkey'
    webhook: 'orchestrator.raijin.users.default.slack.webhookurl'
    DEA_MODULE: dea/20181213
    PROJECT: v10
    QUEUE: normal
  region: ap-southeast-2
  deploymentBucket: ${self:custom.CustEnv.s3Bucket.${self:custom.Stage}}
  stackTags:
    repo: https://github.com/GeoscienceAustralia/dea-orchestration
    author: nci.monitor@dea.ga.gov.au
    purpose: nci-automation
  iamRoleStatements:
    - Effect: 'Allow'
      Action:
        - 'ssm:GetParameters'
        - 'ssm:DescribeParameters'
      Resource:
        - "arn:aws:ssm:#{AWS::Region}:#{AWS::AccountId}:parameter/orchestrator.*"
        - "arn:aws:ssm:#{AWS::Region}:#{AWS::AccountId}:parameter/pipeline.*"
    - Effect: 'Allow'
      Action: 'kms:Decrypt'
      Resource:
        - "arn:aws:kms:#{AWS::Region}:#{AWS::AccountId}:key/*"

functions:
  git_pull_prod:
    handler: handler.execute_ssh_command
    environment:
      hostkey: 'orchestrator.raijin.users.git_pull.host'
      userkey: 'orchestrator.raijin.users.git_pull.user'
      pkey: 'orchestrator.raijin.users.git_pull.pkey'
    events:
      - schedule:
          rate: cron(00 10 ? * FRI,SAT,SUN *)  # Run every Friday-Sunday, at 08:00 pm Canberra time
          enabled: true
  execute_sync:
    # trasharchived is set to 'yes' only for the albers products and not for the scenes
    handler: handler.execute_ssh_command
    description: Sync Landsat 7/8 Scenes (NBAR/NBART/PQ/PQ_LEGACY Scenes) and Sentinal 2 ARD Granules
    environment:
      cmd: 'execute_sync --dea-module ${self:provider.environment.DEA_MODULE}
                         --queue ${self:provider.environment.QUEUE}
                         --project ${self:provider.environment.PROJECT}
                         --year <%= year %>
                         --path <%= path %>
                         --suffixpath <%= suffixpath %>
                         --product <%= product %>
                         --trasharchived <%= trasharchived %>'
    events:
      - schedule:
          rate: cron(10 10 ? * FRI *)  # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: ls8_nbar_scene
            trasharchived: no
            year: '2016-2019'
            path: '/g/data/rs0/scenes/nbar-scenes-tmp/ls8/'
            suffixpath: '/??/output/nbar/'
      - schedule:
          rate: cron(10 10 ? * FRI *)  # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: ls7_nbar_scene
            trasharchived: no
            year: '2016-2019'
            path: '/g/data/rs0/scenes/nbar-scenes-tmp/ls7/'
            suffixpath: '/??/output/nbar/'
      - schedule:
          rate: cron(10 10 ? * FRI *)  # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: ls8_nbart_scene
            trasharchived: no
            year: '2016-2019'
            path: '/g/data/rs0/scenes/nbar-scenes-tmp/ls8/'
            suffixpath: '/??/output/nbart/'
      - schedule:
          rate: cron(10 10 ? * FRI *)  # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: ls7_nbart_scene
            trasharchived: no
            year: '2016-2019'
            path: '/g/data/rs0/scenes/nbar-scenes-tmp/ls7/'
            suffixpath: '/??/output/nbart/'
      - schedule:
          rate: cron(10 10 ? * FRI *)  # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: ls8_pq_scene
            trasharchived: no
            year: '2016-2019'
            path: '/g/data/rs0/scenes/pq-scenes-tmp/ls8/'
            suffixpath: '/??/output/pqa/'
      - schedule:
          rate: cron(10 10 ? * FRI *)  # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: ls7_pq_scene
            trasharchived: no
            year: '2016-2019'
            path: '/g/data/rs0/scenes/pq-scenes-tmp/ls7/'
            suffixpath: '/??/output/pqa/'
      - schedule:
          rate: cron(10 10 ? * FRI *)  # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: ls8_pq_legacy_scene
            trasharchived: no
            year: '2016-2019'
            path: '/g/data/rs0/scenes/pq-legacy-scenes-tmp/ls8/'
            suffixpath: '/??/output/pqa/'
      - schedule:
          rate: cron(10 10 ? * FRI *)  # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: ls7_pq_legacy_scene
            trasharchived: no
            year: '2016-2019'
            path: '/g/data/rs0/scenes/pq-legacy-scenes-tmp/ls7/'
            suffixpath: '/??/output/pqa/'
      - schedule:
          rate: cron(10 10 ? * THU *)  # Run every Thursday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-01-'  # Process January month data
      - schedule:
          rate: cron(10 10 ? * THU *)  # Run every Thursday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-02-'  # Process February month data
      - schedule:
          rate: cron(10 10 ? * THU *)  # Run every Thursday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-03-'  # Process March month data
      - schedule:
          rate: cron(10 10 ? * THU *)  # Run every Thursday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-04-'  # Process April month data
      - schedule:
          rate: cron(10 10 ? * THU *)  # Run every Thursday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-05-'  # Process May month data
      - schedule:
          rate: cron(10 10 ? * THU *)  # Run every Thursday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-06-'  # Process June month data
      - schedule:
          rate: cron(10 10 ? * THU *)  # Run every Thursday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-07-'  # Process July month data
      - schedule:
          rate: cron(10 10 ? * THU *)  # Run every Thursday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-08-'  # Process August month data
      - schedule:
          rate: cron(10 10 ? * THU *)  # Run every Thursday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-09-'  # Process September month data
      - schedule:
          rate: cron(10 10 ? * THU *)  # Run every Thursday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-10-'  # Process October month data
      - schedule:
          rate: cron(10 10 ? * THU *)  # Run every Thursday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-11-'  # Process November month data
      - schedule:
          rate: cron(10 10 ? * THU *)  # Run every Thursday, at 08:10 pm Canberra time
          enabled: true
          input:
            product: s2_ard_granule
            trasharchived: no
            year: '2019-2019'
            path: '/g/data/if87/datacube/002/S2_MSI_ARD/packaged/'
            suffixpath: '-12-'  # Process December month data
  execute_ingest:
    handler: handler.execute_ssh_command
    description: Execute dea-submit-ingest qsub tool to ingest datasets to the database
    environment:
      cmd: 'execute_ingest --dea-module ${self:provider.environment.DEA_MODULE}
                           --queue ${self:provider.environment.QUEUE}
                           --project ${self:provider.environment.PROJECT}
                           --stage ${self:custom.Stage}
                           --year <%= year %>
                           --product <%= product %>'
    events:
      - schedule:
          rate: cron(15 10 ? * SAT *)  # Run every Saturday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2019
            product: ls8_nbart_albers
      - schedule:
          rate: cron(15 10 ? * SAT *)  # Run every Saturday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2019
            product: ls8_nbar_albers
      - schedule:
          rate: cron(15 10 ? * SAT *)  # Run every Saturday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2019
            product: ls8_pq_albers
      - schedule:
          rate: cron(15 10 ? * SAT *)  # Run every Saturday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2019
            product: ls7_nbart_albers
      - schedule:
          rate: cron(15 10 ? * SAT *)  # Run every Saturday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2019
            product: ls7_nbar_albers
      - schedule:
          rate: cron(15 10 ? * SAT *)  # Run every Saturday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2019
            product: ls7_pq_albers
  execute_fractional_cover:
    handler: handler.execute_ssh_command
    description: Execute datacube-fc submit tool to generate fc and ingest them to the database
    environment:
      cmd: 'execute_fractional_cover --dea-module ${self:provider.environment.DEA_MODULE}
                                     --queue ${self:provider.environment.QUEUE}
                                     --project ${self:provider.environment.PROJECT}
                                     --stage ${self:custom.Stage}
                                     --year <%= year %>
                                     --product <%= product %>
                                     --tag <%= tag %>'
    events:
      - schedule:
          rate: cron(15 10 ? * SUN *)  # Run every Sunday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2019
            product: ls8_fc_albers
            tag: 'ls8_fc2019'
      - schedule:
          rate: cron(15 10 ? * SUN *)  # Run every Sunday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2019
            product: ls7_fc_albers
            tag: 'ls7_fc2019'
  execute_wofs:
    handler: handler.execute_ssh_command
    description: Execute datacube-wofs submit tool to generate wofs and ingest them to the database
    environment:
      cmd: 'execute_wofs --dea-module ${self:provider.environment.DEA_MODULE}
                         --queue ${self:provider.environment.QUEUE}
                         --project ${self:provider.environment.PROJECT}
                         --stage ${self:custom.Stage}
                         --year <%= year %>
                         --product <%= product %>
                         --tag <%= tag %>'
    events:
      - schedule:
          rate: cron(20 10 ? * SUN *)  # Run every Sunday, at 08:20 pm Canberra time
          enabled: true
          input:
            year: 2019
            product: wofs_albers
            tag: '2019'
  execute_coherence:
    # Note: Keep `--timequery` argument as the last argument in the environment.cmd
    handler: handler.execute_ssh_command
    description: Execute dea-coherence tool to report archived/locationless parent and their derived datasets
    environment:
      cmd: 'execute_coherence --dea-module ${self:provider.environment.DEA_MODULE}
                              --queue ${self:provider.environment.QUEUE}
                              --project ${self:provider.environment.PROJECT}
                              --product <%= product %>
                              --timequery <%= timequery %>'
    events:
      - schedule:
          rate: cron(00 09 ? * TUE *)  # Run every Tuesday, at 07:00 pm Canberra time
          enabled: true
          input:
            timequery: 'time in 2019'
            product: all
  execute_notify_on_error_ds:
    handler: handler.execute_ssh_command
    description: Detect CSV file containing discrepencies in database, upload CSV to S3 and notify (Slack and Email)
    environment:
      cmd: 'execute_notify_on_error_ds --dea-module ${self:provider.environment.DEA_MODULE}
                                       --webhook  ${self:provider.environment.webhook}
                                       --awsprofile <%= awsprofile %>
                                       --recemail <%= recemail %>'


    events:
      - schedule:
          rate: cron(00 12 ? * * *)  # Run every day, at 10:00 pm Canberra time
          enabled: true
          input:
            awsprofile: default
            recemail: nci.monitor@dea.ga.gov.au  # Recepient's email address
  execute_clean:
    # Note: Keep `--search-paths` argument as the last argument in the environment.cmd
    handler: handler.execute_ssh_command
    description: Execute dea-clean tool to list things to be cleaned, not go and automatically delete files
    environment:
      cmd: 'execute_clean --dea-module ${self:provider.environment.DEA_MODULE}
                          --queue ${self:provider.environment.QUEUE}
                          --project ${self:provider.environment.PROJECT}
                          --min-trash-age <%= min_trash_age %>
                          --search-paths <%= search_paths %>'
    events:
      - schedule:
          rate: cron(00 13 ? * TUE *)  # Run every Tuesday, at 11:00 pm Canberra time
          enabled: true
          input:
            min_trash_age: 72  # 72 hours
            search_paths: /g/data/rs0/datacube/002/
                          /g/data/fk4/datacube/002/FC/
                          /g/data/fk4/datacube/002/WOfS/WOfS_25_2_1/netcdf/
  execute_s2ard:
    handler: handler.execute_ssh_command
    description: Entrypoints for Sentinel-2 level-1 yaml generation and level-2 processing
    environment:
      cmd: 'execute_s2ard --project ${self:provider.environment.PROJECT}
                          --level1-dir <%= level1_dir %>
                          --output-dir <%= output_dir %>
                          --copy-parent-dir-count <%= copy_parent_dir_count %>
                          --file-mod-start <% print(new Date().toISOString().split("T")[0]) %>
                          --file-mod-start-offset <%= start_date_offset %>
                          --days-to-process <%= days_to_process %>
                          --task <%= task %>
                          --obs-year <%= obs_year %>'
    events:
      - schedule:
          rate: cron(5 1 ? * TUE *) # Run weekly at 11:05AM AEST
          enabled: true
          input:
            task: level1
            start_date_offset: 7 # Process last 7 days up until 00:00 Tuesday
            days_to_process: 7
            output_dir: /g/data/v10/AGDCv2/datacube-ingestion/indexed-products/cophub/s2/s2_l1c_yamls
            copy_parent_dir_count: 1
            level1_dir: /g/data/fj7/Copernicus/Sentinel-2/MSI/L1C
            obs_year: current
      - schedule:
          rate: cron(10 1 1 * ? *) # Run on the 1st of every month at 11:10 AM AEST
          enabled: true
          input:
            task: level1
            start_date_offset: 31 # Process last 31 days up until 00:00 TODAY
            days_to_process: 31
            output_dir: /g/data/v10/AGDCv2/datacube-ingestion/indexed-products/cophub/s2/s2_l1c_yamls
            copy_parent_dir_count: 1
            level1_dir: /g/data/fj7/Copernicus/Sentinel-2/MSI/L1C
            obs_year: previous
      - schedule:
          rate: cron(5 1 ? * SAT *) # Run every Saturday at 11:05 AM, AEST
          enabled: true
          input:
            task: level2
            start_date_offset: 12 # Process from Monday 00:00 to Monday 00:00
            days_to_process: 7
            output_dir: /g/data/if87/datacube/002/S2_MSI_ARD/packaged
            copy_parent_dir_count: 0  # IGNORED FOR L2
            level1_dir: /g/data/fj7/Copernicus/Sentinel-2/MSI/L1C
            obs_year: current
      - schedule:
          rate: cron(5 5 5 * ? *) # Run monthly on the 5th, kick off find command at 3:05pm
          enabled: true
          input:
            task: level2
            start_date_offset: 35 # Kicks off re-processing for the past month for previous years on the 5th
            days_to_process: 31
            output_dir: /g/data/if87/datacube/002/S2_MSI_ARD/packaged
            copy_parent_dir_count: 0  # IGNORED FOR L2
            level1_dir: /g/data/fj7/Copernicus/Sentinel-2/MSI/L1C
            obs_year: previous
  execute_cog_conversion:
    handler: handler.execute_ssh_command
    description: Execute COG-Conversion process for the specified product and upload the files back to S3
    environment:
      cmd: 'execute_cog_conversion --dea-module ${self:provider.environment.DEA_MODULE}
                                   --queue ${self:provider.environment.QUEUE}
                                   --project ${self:provider.environment.PROJECT}
                                   --cog-product <%= cog_product %>
                                   --time-range <%= time_range %>'
    events:
      - schedule:
          rate: cron(30 10 ? * MON *)  # Run every Monday, at 08:30 pm Canberra time
          enabled: true
          input:
            cog_product: wofs_albers
            time_range: '2010-2019'
