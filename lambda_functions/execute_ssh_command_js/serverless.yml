# Welcome to Serverless!
#
# This file is the main config file for your service.
#
# For full config options, check the docs:
#    docs.serverless.com
#
# Happy Coding!
service: execute-on

# You can pin your service to only deploy with a specific Serverless version
# Check out our docs for more details
# frameworkVersion: "=X.X.X"

# plugins:
plugins:
  # install serverless pseudo parameters using the following:
  #   1) $ npm install serverless-pseudo-parameters --save-dev
  - serverless-pseudo-parameters

custom:
  Stage: ${opt:stage, self:provider.stage}
  profiles:
    dev: devProfile
    prod: prodProfile
  CustEnv:
    s3Bucket:
      prod: "dea-lambda"
      dev: "dea-lambdas-dev"

provider:
  name: aws
  runtime: nodejs6.10
  timeout: 300 # 5 minutes. Default is 6 seconds
  profile: ${self:custom.profiles.${self:custom.Stage}}
  environment:
    hostkey: 'orchestrator.raijin.users.default.host'
    userkey: 'orchestrator.raijin.users.default.user'
    pkey: 'orchestrator.raijin.users.default.pkey'
    webhook: 'orchestrator.raijin.users.default.slack.webhookurl'
    DEA_MODULE: dea/20181015
    PROJECT: v10
    QUEUE: normal
  region: ap-southeast-2
  deploymentBucket: ${self:custom.CustEnv.s3Bucket.${self:custom.Stage}}
  stackTags:
    repo: dea-orchestration
    author: nci.monitor@dea.ga.gov.au
    purpose: nci-automation
# you can add statements to the Lambda function's IAM Role here
  iamRoleStatements:
    - Effect: 'Allow'
      Action:
        - 'ssm:GetParameters'
        - 'ssm:DescribeParameters'
      Resource:
        - "arn:aws:ssm:#{AWS::Region}:#{AWS::AccountId}:parameter/orchestrator.*"
        - "arn:aws:ssm:#{AWS::Region}:#{AWS::AccountId}:parameter/pipeline.*"
    - Effect: 'Allow'
      Action: 'kms:Decrypt'
      Resource:
        - "arn:aws:kms:#{AWS::Region}:#{AWS::AccountId}:key/*"

functions:
  git_pull_prod:
    handler: handler.execute_ssh_command
    environment:
      hostkey: 'orchestrator.raijin.users.git_pull.host'
      userkey: 'orchestrator.raijin.users.git_pull.user'
      pkey: 'orchestrator.raijin.users.git_pull.pkey'
    events:
      - schedule:
          rate: cron(00 10 ? * FRI,SAT,SUN *) # Run every Friday-Sunday, at 08:00 pm Canberra time
          enabled: true
  execute_sync:
    # trasharchived is set to 'yes' only for the albers products and not for the scenes
    handler: handler.execute_ssh_command
    description: Execute dea-sync tool to sync datasets from the specified path
    environment:
      cmd: 'execute_sync --dea-module ${self:provider.environment.DEA_MODULE}
                         --queue ${self:provider.environment.QUEUE}
                         --project ${self:provider.environment.PROJECT}
                         --stage ${self:custom.Stage}
                         --year <%= year %>
                         --path <%= path %>
                         --product <%= product %>
                         --trasharchived <%= trasharchived %>'
    events:
      - schedule:
          rate: cron(10 10 ? * FRI *) # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls8_pq_scene
            path: '/g/data/rs0/scenes/pq-scenes-tmp/ls8/2018/'
            trasharchived: no
      - schedule:
          rate: cron(10 10 ? * FRI *) # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls8_pq_legacy_scene
            path: '/g/data/rs0/scenes/pq-legacy-scenes-tmp/ls8/2018/'
            trasharchived: no
      - schedule:
          rate: cron(10 10 ? * FRI *) # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls8_nbart_scene
            path: '/g/data/rs0/scenes/nbar-scenes-tmp/ls8/2018/??/output/nbart/'
            trasharchived: no
      - schedule:
          rate: cron(10 10 ? * FRI *) # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls8_nbar_scene
            path: '/g/data/rs0/scenes/nbar-scenes-tmp/ls8/2018/??/output/nbar/'
            trasharchived: no
      - schedule:
          rate: cron(10 10 ? * FRI *) # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls7_pq_scene
            path: '/g/data/rs0/scenes/pq-scenes-tmp/ls7/2018/'
            trasharchived: no
      - schedule:
          rate: cron(10 10 ? * FRI *) # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls7_pq_legacy_scene
            path: '/g/data/rs0/scenes/pq-legacy-scenes-tmp/ls7/2018/'
            trasharchived: no
      - schedule:
          rate: cron(10 10 ? * FRI *) # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls7_nbart_scene
            path: '/g/data/rs0/scenes/nbar-scenes-tmp/ls7/2018/??/output/nbart/'
            trasharchived: no
      - schedule:
          rate: cron(10 10 ? * FRI *) # Run every Friday, at 08:10 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls7_nbar_scene
            path: '/g/data/rs0/scenes/nbar-scenes-tmp/ls7/2018/??/output/nbar/'
            trasharchived: no
  execute_ingest:
    handler: handler.execute_ssh_command
    description: Execute dea-submit-ingest qsub tool to ingest datasets to the database
    environment:
      cmd: 'execute_ingest --dea-module ${self:provider.environment.DEA_MODULE}
                           --queue ${self:provider.environment.QUEUE}
                           --project ${self:provider.environment.PROJECT}
                           --stage ${self:custom.Stage}
                           --year <%= year %>
                           --product <%= product %>'
    events:
      - schedule:
          rate: cron(15 10 ? * SAT *) # Run every Saturday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls8_nbart_albers
      - schedule:
          rate: cron(15 10 ? * SAT *) # Run every Saturday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls8_nbar_albers
      - schedule:
          rate: cron(15 10 ? * SAT *) # Run every Saturday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls8_pq_albers
      - schedule:
          rate: cron(15 10 ? * SAT *) # Run every Saturday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls7_nbart_albers
      - schedule:
          rate: cron(15 10 ? * SAT *) # Run every Saturday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls7_nbar_albers
      - schedule:
          rate: cron(15 10 ? * SAT *) # Run every Saturday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls7_pq_albers
  execute_fractional_cover:
    handler: handler.execute_ssh_command
    description: Execute datacube-fc submit tool to generate fc and ingest them to the database
    environment:
      cmd: 'execute_fractional_cover --dea-module ${self:provider.environment.DEA_MODULE}
                                     --queue ${self:provider.environment.QUEUE}
                                     --project ${self:provider.environment.PROJECT}
                                     --stage ${self:custom.Stage}
                                     --year <%= year %>
                                     --product <%= product %>
                                     --tag <%= tag %>'
    events:
      - schedule:
          rate: cron(15 10 ? * SUN *) # Run every Sunday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls8_fc_albers
            tag: 'ls8_fc2018'
      - schedule:
          rate: cron(15 10 ? * SUN *) # Run every Sunday, at 08:15 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: ls7_fc_albers
            tag: 'ls7_fc2018'
  execute_wofs:
    handler: handler.execute_ssh_command
    description: Execute datacube-wofs submit tool to generate wofs and ingest them to the database
    environment:
      cmd: 'execute_wofs --dea-module ${self:provider.environment.DEA_MODULE}
                         --queue ${self:provider.environment.QUEUE}
                         --project ${self:provider.environment.PROJECT}
                         --stage ${self:custom.Stage}
                         --year <%= year %>
                         --product <%= product %>
                         --tag <%= tag %>'
    events:
      - schedule:
          rate: cron(20 10 ? * SUN *) # Run every Sunday, at 08:20 pm Canberra time
          enabled: true
          input:
            year: 2018
            product: wofs_albers
            tag: '2018'
  execute_coherence:
    # execute_coherence shall not be used on nbar/nbart/pq albers products
    handler: handler.execute_ssh_command
    description: Execute dea-coherence tool to check the database inconsistencies (duplicates, siblings, locationless)
    environment:
      cmd: 'execute_coherence --dea-module ${self:provider.environment.DEA_MODULE}
                              --queue ${self:provider.environment.QUEUE}
                              --project ${self:provider.environment.PROJECT}
                              --timequery "<%= timequery %>"'
    events:
      - schedule:
          rate: cron(00 09 ? * * *) # Run every day, at 07:00 pm Canberra time
          enabled: true
          input:
            timequery: 'time in 2018'
  execute_notify_on_error_ds:
    handler: handler.execute_ssh_command
    description: Detect CSV file containing discrepencies in database, upload CSV to S3 and notify (Slack and Email)
    environment:
      cmd: 'execute_notify_on_error_ds --dea-module ${self:provider.environment.DEA_MODULE}
                                       --webhook  ${self:provider.environment.webhook}
                                       --awsprofile <%= awsprofile %>
                                       --recemail <%= recemail %>'


    events:
      - schedule:
          rate: cron(00 12 ? * * *) # Run every day, at 10:00 pm Canberra time
          enabled: true
          input:
            awsprofile: default
            recemail: nci.monitor@dea.ga.gov.au # Recepient's email address
  execute_clean:
    handler: handler.execute_ssh_command
    description: Execute dea-clean tool to clean the file on the disk
    environment:
      cmd: 'execute_clean --dea-module ${self:provider.environment.DEA_MODULE}
                          --queue ${self:provider.environment.QUEUE}
                          --project ${self:provider.environment.PROJECT}
                          --min-trash-age-hours <%= mintrashage %>
                          --search-string <%= searchstr %>'
    events:
      - schedule:
          rate: cron(00 11 ? * SUN *) # Run every Sunday, at 09:00 pm Canberra time
          enabled: false
          input:
            mintrashage: 10
            searchstr: ls8_nbar_albers
  execute_s2ard:
    handler: handler.execute_ssh_command
    environment:
      cmd: 'execute_s2ard --project ${self:provider.environment.PROJECT}
                          --level1-dir <%= level1_dir %>
                          --output-dir <%= output_dir %>
                          --copy-parent-dir-count <%= copy_parent_dir_count %>
                          --file-mod-start <% print(new Date().toISOString().split("T")[0]) %>
                          --file-mod-start-offset <%= start_date_offset %>
                          --days-to-process <%= days_to_process %>
                          --task <%= task %>
                          --obs-year <%= obs_year %>'
    events:
      - schedule:
          rate: cron(17 05 ? * FRI *) # Run every Friday at 3:05 AM, AEST
          enabled: false
          input:
            task: level1
            start_date_offset: 8 # TODAY - 1 day buffer - 7 days to process
            days_to_process: 7
            output_dir: /g/data/v10/AGDCv2/datacube-ingestion/indexed-products/cophub/s2/s2_l1c_yamls
            copy_parent_dir_count: 1
            level1_dir: /g/data/fj7/Copernicus/Sentinel-2/MSI/L1C
            obs_year: current
      - schedule:
          rate: cron(17 10 28 * ? *) # Run on the 28th of every month at 3:10 AM, AEST
          enabled: false
          input:
            task: level1
            start_date_offset: 32 # TODAY - 1 day buffer - 31 days to process
            days_to_process: 31
            output_dir: /g/data/v10/AGDCv2/datacube-ingestion/indexed-products/cophub/s2/s2_l1c_yamls
            copy_parent_dir_count: 1
            level1_dir: /g/data/fj7/Copernicus/Sentinel-2/MSI/L1C
            obs_year: previous
      - schedule:
          rate: cron(17 05 ? * FRI *) # Run every Friday at 3:05 AM, AEST
          enabled: false
          input:
            task: level2
            start_date_offset: 35 # TODAY - 28 day buffer - 7 days to process
            days_to_process: 7
            output_dir: /g/data/if87/datacube/002/S2_MSI_ARD/packaged
            copy_parent_dir_count: 0 # IGNORED FOR L2
            level1_dir: /g/data/fj7/Copernicus/Sentinel-2/MSI/L1C
            obs_year: current
      - schedule:
          rate: cron(17 10 28 * ? *) # Run on the 28th of every month at 3:10 AM, AEST
          enabled: false
          input:
            task: level2
            start_date_offset: 59 # TODAY - 28 day buffer - 31 days to process
            days_to_process: 31
            output_dir: /g/data/if87/datacube/002/S2_MSI_ARD/packaged
            copy_parent_dir_count: 0 # IGNORED FOR L2
            level1_dir: /g/data/fj7/Copernicus/Sentinel-2/MSI/L1C
            obs_year: previous
