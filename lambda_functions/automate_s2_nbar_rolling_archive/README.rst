.. role:: bash(code)
   :language: bash

------------
 Background
------------
The **Sentinel-2 Definitive NBAR** product is a rolling archive stored on AWS S3 for the last two years.
This process requires *serverless* automation to kick start weekly generation of new datasets once *S2 ARD Granules*
are successfully generated by *execute_ssh_command_js* *serverless* lambda function.

# Required Manual steps

Once the *S2 ARD Granule* generation process is completed, we need to run the `wagl` `batch_summary` command to generate list of successfully processed *S2 level1* datasets. This list is a text file, with each line being a *level1 S2 Zip file*, that has been
processed through to *S2 ARD* pipelines.

.. code:: bash

  [ raijin ]$ cd /
  [ raijin ]$ module use /g/data/v10/public/modules/modulefiles/
  [ raijin ]$ module use /g/data/v10/private/modules/modulefiles/
  [ raijin ]$ module load wagl/5.4.0
  [ raijin ]$ batch_summary --indir /g/data/v10/work/s2_ard/wagl/<batch dir name>/ --outdir=/g/data/v10/work/s2_ard/wagl/<batch dir name>/


It is then necessary to update the *LEVEL1_DONE_LIST* environment variable in the *automate_s2_nbar_rolling_archive/serverless.yml* file, to point to the above text file.

=======================
How the process works
=======================

Lambda Function
===============

- Lambda function is scheduled to run weekly. Once the list is completely processed, the lambda function will rename
  the file so that it won't be processed again.
- To access *NCI* */g/data* for *serverless* orchestration, the lambda function (*execute_s2nbar/handler.py*) will run *raijin_scripts/execute_s2nbar_rolling_archive* *raijin* script *raijin_scripts* folder.

Raijin Script
==============
Raijin script *raijin_scripts/execute_s2nbar_rolling_archive* *raijin* from Raijin scripts folder has necessary permission to run under one of DEA's NCI accounts. Commands in this folder are locked down to ensure that the user isn't able to
execute arbitrary code in our environment.

1. *raijin_scripts/execute_s2nbar_rolling_archive* script simply submits a PBS job which executes *execute_submit_s2_copyq_jobs*

2. *execute_submit_s2_copyq_jobs* PBS script:
    - Runs on a *_copyq_* node to access S3
    - Reads from the list of *level1* datasets that have been successfully processed
    - Turns each level 1 file name into the path to an ARD dataset
    - Uploads everything from path to an ARD dataset, except *NBAR-T*
    - Uploads the updated dataset document. Updates to the dataset document include:
        - Removes lineage
        - Removes NBAR-T measurements
        - Generate a new Dataset ID
        - Update *creation_dt* field
        - Update *product_type* field
